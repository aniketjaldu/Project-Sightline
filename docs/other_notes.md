# Other Notes and Documents

## Project Progress Summary
- The team began by cutting and annotating gameplay clips, then iteratively refined the annotation process and project scope.
- Early work included both REPLAY and LIVE round clips, but the team switched to only using LIVE rounds for consistency.
- The dataset was organized by map and round type, and metadata was added to improve training and evaluation.
- The team experimented with both static and dynamic element labeling, ultimately focusing on dynamic elements for better model relevance.
- Custom tracking configurations and pipeline improvements were made to address challenges with tracking and annotation.
- The pipeline was built to be flexible, supporting rapid changes in direction and workflow.
- The team regularly reviewed and corrected annotations, and explored ways to speed up labeling.
- The project concluded with a robust, modular pipeline and a well-organized dataset and workflow structure.

## Lessons Learned
- Flexibility in pipeline design is crucial for research projects with evolving requirements.
- Annotation is often more time-consuming than anticipated, especially for complex, dynamic scenes.
- Consistent data and metadata are key to effective model training and evaluation.
- Scheduling breaks and maintaining team communication helps sustain long-term progress.

## Additional Context
- All scripts, configs, and documentation are versioned and organized for easy review.
- For any questions or further information, see the README or contact the project team. 